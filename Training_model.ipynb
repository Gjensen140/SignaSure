{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L4P-ZHDcovm0"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import skimage.io as sk\n",
        "from skimage import img_as_ubyte\n",
        "from skimage.io import imread\n",
        "from scipy import spatial\n",
        "from tensorflow.keras.layers import Dense, Flatten, Input, Lambda, MaxPooling2D, Conv2D, Dropout, BatchNormalization\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img\n",
        "from keras.models import Sequential\n",
        "from glob import glob\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PtjU70p1eDd2"
      },
      "outputs": [],
      "source": [
        "train_path = 'sign_data/train'\n",
        "test_path = 'sign_data/test'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6PCqK8I34GLN"
      },
      "outputs": [],
      "source": [
        "Image_Width = 512\n",
        "Image_Height = 512\n",
        "Image_Size = (Image_Width, Image_Height)\n",
        "Image_Channel = 3\n",
        "batch_size=15"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Image settings and batch size\n",
        "Image_Width, Image_Height, Image_Channel = 224, 224, 3\n",
        "batch_size = 32\n",
        "\n",
        "def get_image_paths_and_labels(base_dir):\n",
        "    image_paths = []\n",
        "    labels = []\n",
        "    \n",
        "    for user_folder in os.listdir(base_dir):\n",
        "        user_path = os.path.join(base_dir, user_folder)\n",
        "        if not os.path.isdir(user_path):\n",
        "            continue\n",
        "        \n",
        "        # Determine label: 1 for forged, 0 for genuine\n",
        "        if \"_forg\" in user_folder:\n",
        "            label = 1\n",
        "        else:\n",
        "            label = 0\n",
        "        \n",
        "        # Collect image paths\n",
        "        for img_name in os.listdir(user_path):\n",
        "            img_path = os.path.join(user_path, img_name)\n",
        "            if img_path.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "                image_paths.append(img_path)\n",
        "                labels.append(label)\n",
        "    \n",
        "    return image_paths, labels\n",
        "\n",
        "# Get training and testing image paths and labels\n",
        "train_image_paths, train_labels = get_image_paths_and_labels(\"sign_data/train\")\n",
        "test_image_paths, test_labels = get_image_paths_and_labels(\"sign_data/test\")\n",
        "\n",
        "# Shuffle the dataset (optional but useful)\n",
        "train_data = list(zip(train_image_paths, train_labels))\n",
        "test_data = list(zip(test_image_paths, test_labels))\n",
        "np.random.shuffle(train_data)\n",
        "np.random.shuffle(test_data)\n",
        "train_image_paths, train_labels = zip(*train_data)\n",
        "test_image_paths, test_labels = zip(*test_data)\n",
        "train_labels = np.array(train_labels)\n",
        "test_labels = np.array(test_labels)\n",
        "\n",
        "def make_pairs(image_paths, labels):\n",
        "    \"\"\"\n",
        "    Create positive and negative pairs.\n",
        "    For each image, we create:\n",
        "      - A positive pair: another image with the same label.\n",
        "      - A negative pair: an image with a different label.\n",
        "    \"\"\"\n",
        "    pairs = []\n",
        "    pair_labels = []\n",
        "    \n",
        "    # Map each label to the list of image paths with that label\n",
        "    label_to_paths = {}\n",
        "    for path, label in zip(image_paths, labels):\n",
        "        label_to_paths.setdefault(label, []).append(path)\n",
        "    \n",
        "    # Create pairs\n",
        "    for i in range(len(image_paths)):\n",
        "        current_path = image_paths[i]\n",
        "        current_label = labels[i]\n",
        "        \n",
        "        # Positive pair (if possible, choose a different image from the same class)\n",
        "        if len(label_to_paths[current_label]) > 1:\n",
        "            possible_pos = [p for p in label_to_paths[current_label] if p != current_path]\n",
        "            pos_path = np.random.choice(possible_pos)\n",
        "            pairs.append((current_path, pos_path))\n",
        "            pair_labels.append(1)\n",
        "        \n",
        "        # Negative pair: choose an image from a different label\n",
        "        negative_labels = [lbl for lbl in label_to_paths.keys() if lbl != current_label]\n",
        "        if negative_labels:\n",
        "            neg_label = np.random.choice(negative_labels)\n",
        "            neg_path = np.random.choice(label_to_paths[neg_label])\n",
        "            pairs.append((current_path, neg_path))\n",
        "            pair_labels.append(0)\n",
        "            \n",
        "    return pairs, np.array(pair_labels)\n",
        "\n",
        "# Create image pairs and corresponding labels for training and testing\n",
        "train_pairs, train_pair_labels = make_pairs(train_image_paths, train_labels)\n",
        "test_pairs, test_pair_labels = make_pairs(test_image_paths, test_labels)\n",
        "\n",
        "def preprocess_pair(image_pair, label):\n",
        "    # Unstack the tensor along the first axis to get the two file paths.\n",
        "    img1_path, img2_path = tf.unstack(image_pair, num=2, axis=0)\n",
        "    \n",
        "    # Preprocess the first image\n",
        "    img1 = tf.io.read_file(img1_path)\n",
        "    img1 = tf.image.decode_jpeg(img1, channels=3)\n",
        "    img1 = tf.image.resize(img1, [Image_Width, Image_Height])\n",
        "    img1 = img1 / 255.0  # Normalize\n",
        "\n",
        "    # Preprocess the second image\n",
        "    img2 = tf.io.read_file(img2_path)\n",
        "    img2 = tf.image.decode_jpeg(img2, channels=3)\n",
        "    img2 = tf.image.resize(img2, [Image_Width, Image_Height])\n",
        "    img2 = img2 / 255.0  # Normalize\n",
        "\n",
        "    return (img1, img2), label\n",
        "\n",
        "# Create TensorFlow datasets from the pairs\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((train_pairs, train_pair_labels))\n",
        "train_ds = train_ds.map(preprocess_pair, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "train_ds = train_ds.shuffle(1000).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((test_pairs, test_pair_labels))\n",
        "test_ds = test_ds.map(preprocess_pair, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "test_ds = test_ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# Now you can train your Siamese network with:\n",
        "# siamese_model.fit(train_ds, validation_data=test_ds, epochs=40)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jDQFljZM_Y60"
      },
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "\n",
        "## Conv layer 1\n",
        "model.add(Conv2D(32, (3,3), activation='relu', input_shape=(Image_Width,Image_Height, Image_Channel)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "## Conv layer 2\n",
        "model.add(Conv2D(64, (3,3), activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "## Conv layer 3\n",
        "model.add(Conv2D(128, (3,3), activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "## Conv layer 4\n",
        "model.add(Conv2D(256, (3,3), activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "## Conv layer 5\n",
        "model.add(Conv2D(256, (3,3), activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "## Conv layer 6\n",
        "model.add(Conv2D(512, (3,3), activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(256,activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Dense(2, activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "dbUTyiUV_hWJ",
        "outputId": "f176bb67-d225-4ce7-e60c-222dbc986625"
      },
      "outputs": [],
      "source": [
        "model.fit(train_ds, validation_data=test_ds, epochs=40)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yhaE-GttM7Ym"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "model.save('forge_2.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from PIL import Image\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Load the trained model\n",
        "model = tf.keras.models.load_model(\"forge_2.h5\")\n",
        "\n",
        "# Compile the model (if not compiled)\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Image settings\n",
        "Image_Width, Image_Height = 224, 224\n",
        "test_dir = \"sign_data/test\"\n",
        "\n",
        "def preprocess_image(image_path):\n",
        "    \"\"\"Preprocesses an image for model inference.\"\"\"\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    image = image.resize((Image_Width, Image_Height))\n",
        "    image = img_to_array(image) / 255.0  # Normalize\n",
        "    return np.expand_dims(image, axis=0)\n",
        "\n",
        "# Prepare test dataset\n",
        "test_images = []\n",
        "test_labels = []\n",
        "\n",
        "for user_folder in os.listdir(test_dir):\n",
        "    user_path = os.path.join(test_dir, user_folder)\n",
        "    if not os.path.isdir(user_path):\n",
        "        continue\n",
        "\n",
        "    label = 1 if \"_forg\" in user_folder else 0  # 0 = Forgery, 1 = Genuine\n",
        "    \n",
        "    for img_name in os.listdir(user_path):\n",
        "        img_path = os.path.join(user_path, img_name)\n",
        "        if img_path.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "            test_images.append(preprocess_image(img_path))\n",
        "            test_labels.append(label)\n",
        "\n",
        "\n",
        "test_images = np.vstack(test_images)\n",
        "test_labels = np.array(test_labels)\n",
        "\n",
        "predictions = model.predict(test_images)\n",
        "predicted_labels = np.argmax(predictions, axis=1)  \n",
        "\n",
        "\n",
        "accuracy = accuracy_score(test_labels, predicted_labels)\n",
        "report = classification_report(test_labels, predicted_labels, target_names=[\"Forgery\", \"Genuine\"])\n",
        "conf_matrix = confusion_matrix(test_labels, predicted_labels)\n",
        "\n",
        "\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "print(\"\\nClassification Report:\\n\", report)\n",
        "print(\"\\nConfusion Matrix:\\n\", conf_matrix)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
        "\n",
        "# For binary classification with softmax, we assume predictions is computed from model.predict(test_images)\n",
        "# and test_labels are the ground-truth labels.\n",
        "# We use the probability for class 1.\n",
        "probabilities = predictions[:, 1]\n",
        "\n",
        "# Define thresholds from 0 to 1 (e.g., 0, 0.01, ..., 1.0)\n",
        "thresholds = np.linspace(0, 1, 101)\n",
        "\n",
        "# Lists to store the computed metrics for each threshold\n",
        "accuracies = []\n",
        "f1_scores = []\n",
        "precisions = []\n",
        "recalls = []\n",
        "fprs = []  # False Positive Rates\n",
        "\n",
        "# Iterate over each threshold value\n",
        "for threshold in thresholds:\n",
        "    # Convert probabilities to binary predictions based on the current threshold\n",
        "    predicted_labels_threshold = (probabilities >= threshold).astype(int)\n",
        "    \n",
        "    # Calculate metrics\n",
        "    acc = accuracy_score(test_labels, predicted_labels_threshold)\n",
        "    f1 = f1_score(test_labels, predicted_labels_threshold)\n",
        "    prec = precision_score(test_labels, predicted_labels_threshold)\n",
        "    rec = recall_score(test_labels, predicted_labels_threshold)\n",
        "    \n",
        "    # Compute confusion matrix and extract true negatives (TN) and false positives (FP)\n",
        "    tn, fp, fn, tp = confusion_matrix(test_labels, predicted_labels_threshold).ravel()\n",
        "    fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
        "    \n",
        "    # Store metrics for later plotting\n",
        "    accuracies.append(acc)\n",
        "    f1_scores.append(f1)\n",
        "    precisions.append(prec)\n",
        "    recalls.append(rec)\n",
        "    fprs.append(fpr)\n",
        "    \n",
        "    # Print the metrics for the current threshold\n",
        "    print(f\"Threshold: {threshold:.2f} | Accuracy: {acc:.4f} | F1: {f1:.4f} | Precision: {prec:.4f} | Recall: {rec:.4f} | FPR: {fpr:.4f}\")\n",
        "\n",
        "# Plot all metrics versus the confidence threshold\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(thresholds, accuracies, label='Accuracy', marker='o')\n",
        "plt.plot(thresholds, f1_scores, label='F1 Score', marker='o')\n",
        "plt.plot(thresholds, precisions, label='Precision', marker='o')\n",
        "plt.plot(thresholds, recalls, label='Recall', marker='o')\n",
        "plt.plot(thresholds, fprs, label='False Positive Rate', marker='o')\n",
        "plt.xlabel(\"Confidence Threshold\")\n",
        "plt.ylabel(\"Metric Value\")\n",
        "plt.title(\"Metrics vs. Confidence Threshold\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyMS9vJrbxvh6AKFzzIvWrlq",
      "collapsed_sections": [],
      "include_colab_link": true,
      "mount_file_id": "1rV1Cn9-hdRfAy8sRoLM6wsVoZFVaLcLO",
      "name": "Real or Forge Signature Detection using Deep Learning.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
